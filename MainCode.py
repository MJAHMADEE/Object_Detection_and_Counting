# -*- coding: utf-8 -*-
"""MJAhmadi_HW3_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X7tnkEuJzrjh8dbz2bqnkiYQnWzqpG19
"""

!nvidia-smi

"""# Load Dataset and Requied_Files

## Download and Prepare Dataset and Requied_Files Folders
"""

!pip install --upgrade --no-cache-dir gdown
!gdown 1q-alwIBf1sYToN5D4SAi5kaVMSemuv-J

import zipfile
zip_file_path = '/content/PASCAL.zip'
folder_path = '/content/PASCAL'
# Extract the zip file to the specified folder
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(folder_path)

import os
file_path = "/content/PASCAL.zip"
if os.path.exists(file_path):
    os.remove(file_path)
    print(f"{file_path} has been deleted successfully.")
else:
    print(f"{file_path} does not exist.")

!pip install --upgrade --no-cache-dir gdown
!gdown 1QheCGZHOZeAQ5MIu-3_4lE-Qf0hu6xX9

import zipfile
zip_file_path = '/content/Requied_Files.zip'
folder_path = '/content/Requied_Files'
# Extract the zip file to the specified folder
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(folder_path)

import os
file_path = "/content/Requied_Files.zip"
if os.path.exists(file_path):
    os.remove(file_path)
    print(f"{file_path} has been deleted successfully.")
else:
    print(f"{file_path} does not exist.")

"""# Train Model

## Training
"""

import sys
sys.path.append('/content/Requied_Files/Requied_Files/')
import torch
import torchvision
import utils
import re
import pascal_dataset, engine, coco_eval, coco_utils, transforms, utils
import torch.utils.data
import torchvision.transforms as T
import utils
from engine import train_one_epoch, evaluate
from pascal_dataset import PASCALDataset
from torch.utils.data import DataLoader
from PIL import Image
import warnings
warnings.filterwarnings("ignore")
from coco_eval import CocoEvaluator
# from dataset import PASCALDataset
from engine import train_one_epoch, evaluate
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from pascal_dataset import PASCALDataset
import utils
from coco_utils import get_coco_api_from_dataset
from coco_eval import CocoEvaluator
import copy
import torch.optim as optim
from torch.optim import lr_scheduler
import shutil
from coco_utils import get_coco_api_from_dataset
from coco_eval import CocoEvaluator
import sys
sys.path.append('/content/Requied_Files/Requied_Files/')
from matplotlib import pyplot as plt
import numpy as np
import matplotlib.patches as mpatches
import pascal_dataset, engine, coco_eval, coco_utils, transforms, utils
import torch.utils.data
import torchvision.transforms as T
import utils
from engine import train_one_epoch, evaluate, _get_iou_types
from pascal_dataset import PASCALDataset
from torch.utils.data import DataLoader
from PIL import Image
import random

# create dataset objects for training, validation, and testing sets
dataset = PASCALDataset('/content/PASCAL/PASCAL/train')
dataset_val = PASCALDataset('/content/PASCAL/PASCAL/val')
dataset_test = PASCALDataset('/content/PASCAL/PASCAL/test')

# create data loader objects for training, validation, and testing sets
data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)

# create an object of the Faster R-CNN model with ResNet50 as backbone and load pretrained weights
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# create an optimizer object with stochastic gradient descent (SGD) algorithm
# and set learning rate, momentum, and weight decay values
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)

# move the model to the appropriate device (GPU if available, otherwise CPU)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

import torch
from pascal_dataset import PASCALDataset
import utils

# Create instances of PASCALDataset for training and validation datasets
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Create data loaders for the training and validation datasets
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

# Define number of epochs and frequency of status updates during training
epoch = 5
print_freq = 25

# Train and validate the model for the specified number of epochs
for epoch in range(epoch):
    # Train the model for one epoch using the train_loader
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate the model using the val_loader
    evaluate(model, val_loader, device)

import torch
import shutil

# Save the state dictionary of the trained model as a file named 'model1.pth'
torch.save(model.state_dict(), 'model1.pth')

# Copy the saved model file from its current location to Google Drive
shutil.copyfile('/content/model1.pth', '/content/drive/My Drive/model1.pth')

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 5 # Replace 1 with the appropriate epoch number
print_freq = 25 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 5 # Replace 1 with the appropriate epoch number
print_freq = 10 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

torch.save(model.state_dict(), 'model1.pth')

shutil.copyfile('/content/model1.pth', '/content/drive/My Drive/model1.pth')

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 5 # Replace 1 with the appropriate epoch number
print_freq = 10 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

# Save model to Drive
torch.save(model.state_dict(), '/content/drive/MyDrive/HWs/HW3/Q2/Models/model2.pth')

_get_iou_types(model)

"""# Test"""

for images, targets in data_loader_test:
    # Move images to the specified device (e.g. GPU) for faster computation
    images = list(image.to(device) for image in images)

    # Move target annotations to the same device
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    # Forward pass of the model to obtain the loss dictionary
    loss_dict = model(images, targets)

# Import the necessary library functions
from torchvision.datasets import CocoDetection
from torchvision.datasets.coco import get_coco_api_from_dataset

# Get the COCO API from the test dataset
coco = get_coco_api_from_dataset(data_loader_test.dataset)

# Define the IoU type(s) for evaluation (here, only "bbox" is used)
iou_types = ["bbox"]

# Instantiate the COCO evaluation object with the specified IoU type(s)
coco_evaluator = CocoEvaluator(coco, iou_types)

# Use the trained model to generate predictions for the input images
outputs = model(images)

# Create a dictionary mapping each image ID to its corresponding prediction output
res = {target["image_id"].item(): output for target, output in zip(targets, outputs)}

# Update the COCO evaluation object with the predicted outputs for the current batch of images
coco_evaluator.update(res)

# Synchronize results across multiple processes for COCO evaluation
coco_evaluator.synchronize_between_processes()

# Accumulate results for COCO evaluation
coco_evaluator.accumulate()

# Summarize COCO evaluation results
coco_evaluator.summarize()

# Accessing the first statistic from the COCO evaluation object for bounding box detection
# The 'coco_evaluator' object is assumed to be an instance of the 'COCOeval' class from the PyCocoTools library
# The 'coco_eval' attribute is a dictionary of COCO evaluation results
# The 'bbox' key accesses the evaluation results for bounding box detection
# The 'stats' attribute is a list of statistics computed during the evaluation
# Indexing with '[0]' returns the first statistic, which is the average precision (AP) across all object categories
coco_evaluator.coco_eval['bbox'].stats[0]

dataset = PASCALDataset('/content/PASCAL/PASCAL/train')
dataset_val = PASCALDataset('/content/PASCAL/PASCAL/val')
dataset_test = PASCALDataset('/content/PASCAL/PASCAL/test')

data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)
# move model to the right device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Define classes list
classes = [
    "__background__",  # 0 index
    "person",
    "bicycle",
    "car",
    "motorbike",
    "aeroplane",
    "bus",
    "bird",
    "cat",
    "chair",
    "cow",
    "diningtable",
    "dog",
    "horse",
    "boat",
    "bottle",
    "pottedplant",
    "sheep",
    "sofa",
    "train",
    "tvmonitor"
]

# Define bbox colors for each class
colors = {
    "person": "red",
    "bicycle": "green",
    "car": "blue",
    "motorbike": "yellow",
    "aeroplane": "cyan",
    "bus": "magenta",
    "bird": "orange",
    "cat": "purple",
    "chair": "brown",
    "cow": "pink",
    "diningtable": "gray",
    "dog": "olive",
    "horse": "teal",
    "boat": "navy",
    "bottle": "maroon",
    "pottedplant": "coral",
    "sheep": "gold",
    "sofa": "lime",
    "train": "indigo",
    "tvmonitor": "darkorange"
}

def plot_image_with_bboxes(image, targets):
    boxes = targets["boxes"]
    labels = targets["labels"]
    fig, ax = plt.subplots(1)
    ax.imshow(image.permute(1,2,0))
    handles = {}
    for label in np.unique(labels):
        class_name = classes[label]
        mask = labels == label
        if label == 0:
            # ignore background class
            continue
        label_boxes = boxes[mask, :]
        for box in label_boxes:
            x1, y1, x2, y2 = box.cpu().numpy()
            edgecolor = colors[class_name]
            if class_name not in handles:
                handles[class_name] = mpatches.Patch(color=edgecolor, label=f'{class_name}: {sum(mask)}')
            ax.add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor=edgecolor, linewidth=2))
    ax.legend(handles=handles.values(), bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.7:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.7:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.7:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='upper right', ncol=len(gt_legend))
    
    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.7:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, bbox_to_anchor=(0.5, -0.1), loc='upper right', ncol=len(pred_legend))
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.7:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.7:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.7:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.8:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

def plot_image_with_bboxes(image, targets, predictions):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    ax1.imshow(image.permute(1, 2, 0))
    ax2.imshow(image.permute(1, 2, 0))
    ax1.set_title('Ground Truth')
    ax2.set_title('Prediction')
    
    # Plot ground truth
    gt_boxes = targets['boxes']
    gt_labels = targets['labels']
    gt_counts = dict()
    for box, label in zip(gt_boxes, gt_labels):
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax1.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        if class_name not in gt_counts:
            gt_counts[class_name] = 1
        else:
            gt_counts[class_name] += 1
    # Add GT legend
    gt_legend = []
    for class_name, count in gt_counts.items():
        gt_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    # ax1.legend(handles=gt_legend, bbox_to_anchor=(0.5, -0.1), loc='right', ncol=len(gt_legend))
    ax1.legend(handles=gt_legend, loc='upper right')

    # Plot predictions
    pred_boxes = predictions['boxes']
    pred_labels = predictions['labels']
    pred_scores = predictions['scores']
    pred_counts = dict()
    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
        if score < 0.75:
            continue
        x1, y1, x2, y2 = box.cpu().numpy()
        class_name = classes[label]
        edgecolor = colors[class_name]
        ax2.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor=edgecolor, linewidth=2))
        ax2.text(x1, y1, f'{class_name}: {score:.2f}', bbox=dict(facecolor='white', alpha=0.5))
        if class_name not in pred_counts:
            pred_counts[class_name] = 1
        else:
            pred_counts[class_name] += 1
    # Add Prediction legend
    pred_legend = []
    for class_name, count in pred_counts.items():
        pred_legend.append(mpatches.Patch(color=colors[class_name], label=f'{class_name} ({count})'))
    ax2.legend(handles=pred_legend, loc='upper right')
    
    plt.savefig('image_with_bboxes.pdf', bbox_inches='tight')
    plt.show()


# Evaluate on 3 random test images
for i in range(1):
    image, target = dataset_test[random.randint(0, len(dataset_test))]
    model.eval()
    with torch.no_grad():
        prediction = model([image.to(device)])[0]
    plot_image_with_bboxes(image, target, prediction)

"""# Other Implementation"""

dataset = PASCALDataset('/content/PASCAL/PASCAL/train')
dataset_val = PASCALDataset('/content/PASCAL/PASCAL/val')
dataset_test = PASCALDataset('/content/PASCAL/PASCAL/test')

data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)
# move model to the right device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 5 # Replace 1 with the appropriate epoch number
print_freq = 10 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

from coco_utils import get_coco_api_from_dataset
from coco_eval import CocoEvaluator

import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from pascal_dataset import PASCALDataset
import utils
from coco_utils import get_coco_api_from_dataset
from coco_eval import CocoEvaluator
import copy
import torch.optim as optim
from torch.optim import lr_scheduler
# from PennFudanDataset import PennFudanDataset

for images, targets in data_loader_test:
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
    loss_dict = model(images, targets)

coco = get_coco_api_from_dataset(data_loader_test.dataset)

iou_types = ["bbox"]

coco_evaluator = CocoEvaluator(coco, iou_types)

outputs = model(images)

res = {target["image_id"].item(): output for target, output in zip(targets, outputs)}

coco_evaluator.update(res)

coco_evaluator.synchronize_between_processes()
coco_evaluator.accumulate()
coco_evaluator.summarize()

coco_evaluator.coco_eval['bbox'].stats[0]

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 1 # Replace 1 with the appropriate epoch number
print_freq = 1 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 1 # Replace 1 with the appropriate epoch number
print_freq = 1 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 1 # Replace 1 with the appropriate epoch number
print_freq = 10 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

from coco_utils import get_coco_api_from_dataset
from coco_eval import CocoEvaluator

import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from pascal_dataset import PASCALDataset
import utils
from coco_utils import get_coco_api_from_dataset
from coco_eval import CocoEvaluator
import copy
import torch.optim as optim
from torch.optim import lr_scheduler
# from PennFudanDataset import PennFudanDataset

for images, targets in data_loader:
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
    loss_dict = model(images, targets)

coco = get_coco_api_from_dataset(data_loader.dataset)

iou_types = ["bbox"]

coco_evaluator = CocoEvaluator(coco, iou_types)

outputs = model(images)

res = {target["image_id"].item(): output for target, output in zip(targets, outputs)}

coco_evaluator.update(res)

coco_evaluator.synchronize_between_processes()
coco_evaluator.accumulate()
coco_evaluator.summarize()

train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None)

train_losses = []
class_losses = []
box_losses = []
obj_losses = []
rpn_losses = []
num_epochs = 1
for epoch in range(num_epochs):
    metric_logger = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Check if the meters have recorded any value before calculating global average
    if metric_logger.meters['loss'].count > 0:
        train_losses.append(metric_logger.meters['loss'].global_avg)
    if metric_logger.meters['class_loss'].count > 0:
        class_losses.append(metric_logger.meters['class_loss'].global_avg)
    if metric_logger.meters['box_loss'].count > 0:
        box_losses.append(metric_logger.meters['box_loss'].global_avg)
    if metric_logger.meters['obj_loss'].count > 0:
        obj_losses.append(metric_logger.meters['obj_loss'].global_avg)
    if metric_logger.meters['rpn_loss'].count > 0:
        rpn_losses.append(metric_logger.meters['rpn_loss'].global_avg)

    # Evaluate on validation set
    evaluate(model, val_loader, device)

import matplotlib.pyplot as plt

train_losses = []
class_losses = []
box_losses = []
obj_losses = []
rpn_losses = []
num_epochs = 1

for epoch in range(num_epochs):
    metric_logger = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    
    # Check if the meters have recorded any value before calculating global average
    if metric_logger.meters['loss'].count > 0:
        train_losses.append(metric_logger.meters['loss'].global_avg)
    if metric_logger.meters['class_loss'].count > 0:
        class_losses.append(metric_logger.meters['class_loss'].global_avg)
    if metric_logger.meters['box_loss'].count > 0:
        box_losses.append(metric_logger.meters['box_loss'].global_avg)
    if metric_logger.meters['obj_loss'].count > 0:
        obj_losses.append(metric_logger.meters['obj_loss'].global_avg)
    if metric_logger.meters['rpn_loss'].count > 0:
        rpn_losses.append(metric_logger.meters['rpn_loss'].global_avg)

    # Evaluate on validation set
    evaluate(model, val_loader, device)

epochs = range(1, num_epochs+1)

# Plot the losses
plt.plot(epochs, train_losses, label='Training Loss')
if class_losses:
    plt.plot(epochs, class_losses, label='Classification Loss')
plt.plot(epochs, box_losses, label='Box Regression Loss')
plt.plot(epochs, obj_losses, label='Objectness Loss')
plt.plot(epochs, rpn_losses, label='RPN Loss')

plt.title('Training Losses')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()


epochs = range(1, num_epochs+1)

# Plot the losses
plt.plot(epochs, train_losses, label='Training Loss')
if class_losses:
    plt.plot(epochs, class_losses, label='Classification Loss')
if box_losses:
    plt.plot(epochs, box_losses, label='Box Regression Loss')
if obj_losses:
    plt.plot(epochs, obj_losses, label='Objectness Loss')
if rpn_losses:
    plt.plot(epochs, rpn_losses, label='RPN Loss')

plt.title('Training Losses')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Define hyperparameters
lr = 0.005
momentum = 0.9
weight_decay = 0.0005
num_epochs = 10
batch_size = 4
num_workers = 4

# Define dataset and data loaders
dataset = PASCALDataset('/content/PASCAL/PASCAL/train')
dataset_val = PASCALDataset('/content/PASCAL/PASCAL/val')
dataset_test = PASCALDataset('/content/PASCAL/PASCAL/test')

data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=utils.collate_fn)
data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=utils.collate_fn)
data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=utils.collate_fn)

# Define the model and optimizer
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)

# Move the model to the right device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Train the model
print("Starting training...")
total_loss = 0
total_loss_values = []
batch_numbers = []
for epoch in range(num_epochs):
    for i, (images, targets) in enumerate(data_loader):
        # Move images and targets to the right device
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Forward pass
        loss_dict = model(images, targets)

        # Compute the total loss
        losses = sum(loss for loss in loss_dict.values())
        total_loss += losses.item()
        total_loss_values.append(losses.item())
        batch_numbers.append(i)

        # Backward pass and optimization step
        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        # Print status every 10 batches
        if i % 10 == 0:
            print(f"Epoch {epoch}, Batch {i}, Loss: {losses.item()}")

    # Evaluate on the validation set after each epoch
    evaluate(model, data_loader_val, device=device)

# Evaluate on the test set
print("Evaluating on the test set...")
evaluate(model, data_loader_test, device=device)

# Train the model
print("Starting training...")
train_losses = []  # Initialize empty list to store losses for each epoch
for epoch in range(1):
    # Train one epoch
    metric_logger, losses = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    train_losses.append(losses)  # Append losses for current epoch to list
    # Evaluate on the validation set
    evaluate(model, data_loader_val, device=device)

import matplotlib.pyplot as plt

# Plot losses for each iteration during training
plt.figure(figsize=(10, 5))
plt.title("Training Loss")
plt.xlabel("Iteration")
plt.ylabel("Loss")
for i, losses in enumerate(train_losses):
    plt.plot(losses, label=f"Epoch {i}")
plt.legend()
plt.show()

train_losses = []
train_maps = []
val_losses = []
val_maps = []

for epoch in range(num_epochs):
    # train for one epoch
    train_loss, train_map = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)
    train_losses.append(train_loss.avg)
    train_maps.append(train_map.avg)
    
    # evaluate on validation set
    val_loss, val_map = evaluate(model, data_loader_val, device)
    val_losses.append(val_loss.avg)
    val_maps.append(val_map.avg)
    
    # print progress
    print(f"Epoch {epoch+1}/{num_epochs}, train_loss: {train_losses[-1]:.4f}, val_loss: {val_losses[-1]:.4f}, train_map: {train_maps[-1]:.4f}, val_map: {val_maps[-1]:.4f}")
    
# Plot the training and validation loss and mAP curves
plot_loss_curves(train_losses, val_losses)
plot_map_curves(train_maps, val_maps)

dataset = PASCALDataset('/content/PASCAL/PASCAL/train')
dataset_val = PASCALDataset('/content/PASCAL/PASCAL/val')
dataset_test = PASCALDataset('/content/PASCAL/PASCAL/test')

data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_val = torch.utils.data.DataLoader(dataset_val, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)

import matplotlib.pyplot as plt
import numpy as np

# initialize lists to store losses
train_losses = []
val_losses = []
class_losses = []
box_losses = []
obj_losses = []
rpn_losses = []

epoch = 1 # Replace 1 with the appropriate epoch number
print_freq = 10 # Print status every 10 iterations

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    train_loss, class_loss, box_loss, obj_loss, rpn_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    train_losses.append(train_loss)
    class_losses.append(class_loss)
    box_losses.append(box_loss)
    obj_losses.append(obj_loss)
    rpn_losses.append(rpn_loss)
    
    # Evaluate on validation set
    val_loss = evaluate(model, val_loader, device)
    val_losses.append(val_loss)
    
    # plot losses
    plt.figure(figsize=(10,5))
    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')
    plt.plot(np.arange(len(val_losses)), val_losses, label='Validation Loss')
    plt.plot(np.arange(len(class_losses)), class_losses, label='Classification Loss')
    plt.plot(np.arange(len(box_losses)), box_losses, label='Box Regression Loss')
    plt.plot(np.arange(len(obj_losses)), obj_losses, label='Objectness Loss')
    plt.plot(np.arange(len(rpn_losses)), rpn_losses, label='RPN Box Regression Loss')
    plt.legend()
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.show()



image, target = dataset_test[random.randint(0, len(dataset_test))]
gt_labels = target['labels']
print(f"Ground truth labels: {gt_labels}")

import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

from pascal_dataset import PASCALDataset
import utils
from engine import train_one_epoch, evaluate

# Define the dataset paths
train_dataset_path = '/content/PASCAL/PASCAL/train'
val_dataset_path = '/content/PASCAL/PASCAL/val'

# Define the batch size and number of epochs
batch_size = 4
num_epochs = 1

# Create the datasets
train_dataset = PASCALDataset(train_dataset_path)
val_dataset = PASCALDataset(val_dataset_path)

# Create the data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

# Load the pre-trained model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# Move the model to the GPU if available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Set up the optimizer
optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)

# Initialize the lists for the train and validation losses
train_losses = []
val_losses = []

# Train the model
for epoch in range(num_epochs):
    # Train for one epoch and collect the train loss
    train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10, scaler=None)
    train_losses.append(train_loss)

    # Evaluate on the validation set and collect the validation loss
    val_loss = evaluate(model, val_loader, device)
    val_losses.append(val_loss)

# Plot and save the losses
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('losses.pdf')

import sys
import torch
import math
import torchvision
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from pascal_dataset import PASCALDataset
from engine import train_one_epoch, evaluate
import utils

# define train and val datasets
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# define data loaders
train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

# define model
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, box_detections_per_img=10)
num_classes = 21  # 20 classes + background
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

# define device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# define optimizer and learning rate scheduler
params = [p for p in model.parameters() if p.requires_grad]
optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

# define training parameters
num_epochs = 10
print_freq = 10

# define lists to store losses
train_losses = []
val_losses = []

# train and validate model for each epoch
for epoch in range(num_epochs):
    # train for one epoch
    train_one_epoch(model, optimizer, train_data_loader, device, epoch, print_freq=print_freq)
    train_loss = utils.flatten_dict(model.train_metrics).get('loss', None)
    train_losses.append(train_loss)

    # evaluate on validation set
    evaluate(model, val_data_loader, device=device)
    val_loss = utils.flatten_dict(model.eval_metrics).get('loss', None)
    val_losses.append(val_loss)

    # update learning rate
    lr_scheduler.step()

# plot and save the losses
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Train and Validation Loss')
plt.legend()
plt.savefig('train_val_loss.pdf')

coco_evaluator.coco_eval['bbox'].stats[0]

# !wget http://people.cs.pitt.edu/~nhonarvar/TA_Spring_2020/PyTorch_Installation_hw3.sh

# !source PyTorch_Installation_hw3.sh

# deactivate

torch.save(model.state_dict(), 'model1.pth')

import shutil
shutil.copyfile('/content/model1.pth', '/content/drive/My Drive/model1.pth')

import matplotlib.pyplot as plt

from pascal_dataset import PASCALDataset

# Training dataset
train_dataset = PASCALDataset('/content/PASCAL/PASCAL/train')

# Validation dataset
val_dataset = PASCALDataset('/content/PASCAL/PASCAL/val')

# Data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=utils.collate_fn)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=utils.collate_fn)

epoch = 1 # Replace 1 with the appropriate epoch number
print_freq = 1 # Print status every 10 iterations


# Define lists to store the losses
loss_list = []
loss_classifier_list = []
loss_box_reg_list = []
loss_objectness_list = []
loss_rpn_box_reg_list = []

# Training loop
for epoch in range(epoch):
    # Train for one epoch
    loss_dict = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq, scaler=None)
    loss = loss_dict['loss'].item()
    loss_classifier = loss_dict['loss_classifier'].item()
    loss_box_reg = loss_dict['loss_box_reg'].item()
    loss_objectness = loss_dict['loss_objectness'].item()
    loss_rpn_box_reg = loss_dict['loss_rpn_box_reg'].item()
    
    # Append losses to lists
    loss_list.append(loss)
    loss_classifier_list.append(loss_classifier)
    loss_box_reg_list.append(loss_box_reg)
    loss_objectness_list.append(loss_objectness)
    loss_rpn_box_reg_list.append(loss_rpn_box_reg)
    
    # Evaluate on validation set
    evaluate(model, val_loader, device)

# Plot and save the losses separately
plt.plot(loss_list)
plt.xlabel('Epoch')
plt.ylabel('Total Loss')
plt.savefig('total_loss.pdf')
plt.show()

plt.plot(loss_classifier_list)
plt.xlabel('Epoch')
plt.ylabel('Classification Loss')
plt.savefig('classification_loss.pdf')
plt.show()

plt.plot(loss_box_reg_list)
plt.xlabel('Epoch')
plt.ylabel('Regression Loss')
plt.savefig('regression_loss.pdf')
plt.show()

plt.plot(loss_objectness_list)
plt.xlabel('Epoch')
plt.ylabel('Objectness Loss')
plt.savefig('objectness_loss.pdf')
plt.show()

plt.plot(loss_rpn_box_reg_list)
plt.xlabel('Epoch')
plt.ylabel('RPN Regression Loss')
plt.savefig('rpn_regression_loss.pdf')
plt.show()

evaluate(model, val_loader, device)

torch.save(model.state_dict(), 'model.pth')

torch.save(model.state_dict(), 'model1.pth')